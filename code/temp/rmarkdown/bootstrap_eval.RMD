---
title: "LogitModel.RMD"
author: "Bart Ciastkowski"
date: "12/2/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
  
# The Code
### Adding libraries
```{r}
library("plyr")
library("conflicted")

library("tidyverse")
library("here")
library("ggthemes")
library("stringr")
library("stringi")
library("readxl")
library("ggExtra")
library("PerformanceAnalytics")
library('sentimentr')

library('partykit')
library('magrittr')
library('analogue')
```

### Resolving library conflicts
```{r}

conflict_prefer("mutate", "dplyr")
conflict_prefer("margin","ggplot2")
```
### Clear all objects including hidden objects.
```{r}
rm(list = ls(all.names = TRUE))
```
### Load Model and Train Data
```{r}
load(here::here("data","output","limited_factors","wine_train.RData"))
load(here::here("data","output","limited_factors","wine_test.RData"))
names(wine_train)
names(wine_test)
```
# Modelling log(price) using decision-tree via Bootstrap technique
### set # of Bootstrap sample to 100 as don't have much that computing horsepower
### set size of each bootstrap to 500 to ensure adequate data
### for ctree model selected specific variables that would produce results (period)
```{r}
# store rownames as columns
wine_train_preds <- wine_train %>% rownames_to_column() %>% 
  mutate(rowname = as.numeric(rowname))

# bagging - bootstrapp aggregation

B <- 100      # number of bootstrap samples
num_b <- 500  # sample size of each bootstrap
boot_mods <- list() # store our bagging models
for(i in 1:B){
  boot_idx <- sample(1:nrow(wine_train), 
                     size = num_b,
                     replace = FALSE)
  # fit a tree on each bootstrap sample
  data_slice = wine_train %>% slice(boot_idx)
  
  # Log(price) bootstrap model ----
  boot_tree <- ctree(log(price) ~ ., 
                     data = data_slice 
  ) 
  # store bootstraped model
  boot_mods[[i]] <- boot_tree
  # generate predictions for that bootstrap model
  preds_boot <- data.frame(
    preds_boot = predict(boot_tree),
    #resid =  data_slice$price - predict(boot_tree),
    rowname = boot_idx 
  )  
  
  
  # rename prediction to indicate which boot iteration it came from
  names(preds_boot)[1] <- paste("preds_boot",i,sep = "")
  
  # merge predictions to wine_train dataset
  wine_train_preds <- left_join(x = wine_train_preds, y = preds_boot,
                                by = "rowname")
}

names(wine_train_preds)
## Examine individual models, will need to spend more time here ----
plot(boot_mods[[1]])
plot(boot_mods[[2]])
plot(boot_mods[[3]])
plot(boot_mods[[5]])
plot(boot_mods[[10]])
plot(boot_mods[[20]])
plot(boot_mods[[40]])
plot(boot_mods[[60]])
plot(boot_mods[[80]])
plot(boot_mods[[100]])

# must convert factor into numeric, note that class "0" = 1, 
# and class "1" = 2, so we need to subtract 1 from every column
wine_train_preds %<>% mutate_if(is.factor, as.numeric) %>% 
  mutate_all(function(x){x - 1})

# calculate mean over all the bootstrap predictions
wine_train_preds %<>% mutate(preds_bag = 
                               select(., preds_boot1:preds_boot100) %>% 
                               rowMeans(na.rm = TRUE))
```
# Plot of the bagged model
### First let plot the log(price) distribution of the wine train dataset
```{r}
## Actual Values
ggplot(wine_train, aes(x = log(price))) + xlim(0, 10) + geom_histogram(binwidth=0.1)
```
### Let's plot the bagged model distribution of the predicted values
```{r}
## vs Bagged model
ggplot(wine_train_preds, aes(x = preds_bag)) + xlim(0, 10) + geom_histogram(binwidth=0.1)
#ggplot(wine_train_preds, aes(x = preds_bag)) + geom_histogram()
#summary(wine_train_preds)
```


# Bootstrap Model Summary
## the variations of the bootstrap predictions appear to sort of match the training dataset
## nonetheless the bootstrap has a bias, a tendency to undervalue wine
## from the review of a number of the decision trees we see that most root and decision nodes use points for decision to split, then variety & color and provice are used 