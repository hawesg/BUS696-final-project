<<<<<<< .mine
---
title: "LogitModel.RMD"
author: "Bart Ciastkowski"
date: "12/2/2019"
output:
  word_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
  

```{r message=FALSE,warning=FALSE,include=FALSE}
# The Code
### Adding libraries

library("plyr")
library("conflicted")

library("tidyverse")
library("here")
library("ggthemes")
library("stringr")
library("stringi")
library("readxl")
library("ggExtra")
library("PerformanceAnalytics")
library("plotROC")
library('sentimentr')

```
  
```{r message=FALSE,warning=FALSE,include=FALSE}
### Resolving library conflicts
conflict_prefer("mutate", "dplyr")
conflict_prefer("margin","ggplot2")
```
  
```{r message=FALSE,warning=FALSE,include=FALSE}
### Clear all objects including hidden objects.
rm(list = ls(all.names = TRUE))
```
  
```{r message=FALSE,warning=FALSE,include=FALSE}
### Load Model and Train Data
#load(here::here("data","output","limited_factors","wine_train.RData"))
#load(here::here("data","output","limited_factors","wine_test.RData"))
load(here::here("data","output","before-class.RData"))

wine_train <- data.train
wine_test <- data.test

names(wine_train)
names(wine_test)
```
  
# Slide 1: Determining whether wine good value or not
## Logit Model
### logit_mod <- glm( well_priced ~ ...... )
## How is Well_Priced determined?
### Well_priced == whether we think wine is well priced
### Well_priced == f(median log(price) to points ratio)
## In Summary: Well_Priced takes into consideration diminishing returns i.e. marginal increase in points is accompanied by a higher and higher increase in price 
```{r message=FALSE,warning=FALSE,include=FALSE}
# Determining Good Value (i.e. well_priced)
### Need to consider diminishing returns, as the rate of marginal increase in points rating decreases with respect to the increase in the price of a bottle of wine, hence using log(price of wine):
##### Well_priced == whether we think wine is well priced
##### Well_priced will be determined according to a median price to points ratio computed as follows using observable from the training data set
##### median_price_to_points_ratio= Dataset ratio numerator  / Dataset ratio denominator
##### Dataset ratio numerator    = Median (points awarded)  - Min (points awarded)
##### Dataset ratio denominator  = Median ( log ( price of wine))  - Min (log (expected(min price of drinkable wine))
##### Where expected(min price of drinkable wine) = $2.5 (i.e. educated guess)
##### Where price of wine > expected (min price of drinkable wine)
### In Summary: the above formula takes into consideration diminishing returns i.e. marginal increase in points is accompanied by a higher and higher increase in price and assumes the lowest price for acceptable bottle of wine to be $2.5 (other numbers could be used in the future) 

wine_train.pt.min <- min(wine_train$points) 
price.min <- 2.5

ratio.numerator <-  median(wine_train$points)-wine_train.pt.min
ratio.denominator <-  ifelse(wine_train$price<price.min, price.min + 0.1, wine_train$price)
ratio.denominator <- log(ratio.denominator)
ratio.denominator <-  median(ratio.denominator)
ratio.denominator <- ratio.denominator - log(price.min)
median_price_to_points_ratio <- ratio.numerator/ratio.denominator

.is_well_priced <- function(df){
  test.LHS.numerator <- df$points-wine_train.pt.min
  test.LHS.denominator <- ifelse(df$price<price.min, price.min + 0.1,df$price)
  test.LHS.denominator <- log(test.LHS.denominator)
  test.LHS.denominator <- test.LHS.denominator-log(price.min)
  test.LHS <- test.LHS.numerator/test.LHS.denominator
  test.RHS <- median_price_to_points_ratio
  well_priced <- factor(test.LHS > test.RHS, labels=c("No", "Yes"))
  return(well_priced)
}


# Compute well_priced for train ---- same formula as for median_price_to_points_ratio, except for an individual price point combination
wine_train_logit <- wine_train %>% 
  dplyr::mutate ( well_priced = .is_well_priced(.) )
```
# Slide 2: Points vs Price by Well Priced 
## Purpose: showing the effects of dimishing returns
```{r message=FALSE,warning=FALSE,include=FALSE}
PointsVsPricePlot <-ggplot(wine_train_logit , aes(x = price, y = points, color = well_priced)) +
  geom_jitter() +
  theme(legend.position = "top") + 
  labs(title="Price and Points Colored by Well Priced", 
       color = "Well Priced") 

#head(wine_train)
```
  
```{r}
PointsVsPricePlot
```

 
```{r message=FALSE,warning=FALSE,include=FALSE}
wine_train_logit <- wine_train_logit %>% 
  dplyr::select(-price,-points.category)

# Compute well_priced for test dataset and remove price and points
wine_test_logit <- wine_test %>% 
  dplyr::mutate ( well_priced = .is_well_priced(.) )  %>% 
  select (-price,-points.category)


### Since well_priced is a binomial estimate of price, price is removed from the dataset (train and test) before model is created
### taster.twitter_handle and variety_and_color are also removed as they provide no added value (i.e. NA values)
### Otherwise same variables are used for the model so that it can be compared against other models
```
  
# Slide 3: Let's create the model
## Notes: Price removed from the dataset as well as variety_and_color as it provided no added value (i.e. NA values)
```{r}
logit_mod <- glm( well_priced ~ .,
                  data = wine_train_logit %>%   
                    select (
                      -variety_and_color, 
                    ),
                  family = binomial) #our varaible can be 0 or 1, a binomial

```
  
```{r message=FALSE,warning=FALSE,include=FALSE}
## Computing predictions
### Train test set
preds.train<- data.frame (
  pred = predict(logit_mod, type="response"),
  wine_train_logit
  #actual = wine_train_logit$well_priced
) 
#head(preds.train)
```
  

```{r message=FALSE,warning=FALSE,include=FALSE}
### Test set
preds.test<- data.frame (
  pred = predict(logit_mod, 
                 newdata=wine_test_logit, 
                 type="response"),
  wine_test_logit
  #actual = wine_test_logit$well_priced
) 
#head(preds.test)

```
  

```{r message=FALSE,warning=FALSE,include=FALSE}
## ROC Curve
TrainDF <- data.frame(default = c(preds.train$well_priced),
                      scores = c(preds.train$pred),
                      models = c(rep("Train Data Set",length(preds.train$pred))))

#summary(TrainDF)

TestDF <- data.frame(default = c(preds.test$well_priced),
                     scores = c(preds.test$pred),
                     models = c(rep("Test Data Set",length(preds.test$pred))))

```

```{r message=FALSE,warning=FALSE,include=FALSE}
### ROC Curve train
TrainROC <- ggplot(TrainDF, aes(m = scores, d = default, color = models)) + 
  geom_roc(show.legend = TRUE, labelsize = 3.5, cutoffs.at = c(.99,.9,.8,.7,.5,.3,.1,0))
TrainROC <- TrainROC + style_roc(theme = theme_grey) +
  theme(axis.text = element_text(colour = "blue")) +
  theme(legend.justification = c(1, 0), 
        legend.position = c(1, 0),
        legend.box.margin=margin(c(50,50,50,50)))
#plot(TrainROC)
```
    
```{r message=FALSE,warning=FALSE,include=FALSE}
### ROC Curve test

TestROC <- ggplot(TestDF, aes(m = scores, d = default, color = models)) + 
  geom_roc(show.legend = TRUE, labelsize = 3.5, cutoffs.at = c(.99,.9,.8,.7,.5,.3,.1,0))
TestROC <- TestROC + style_roc(theme = theme_grey) +
  theme(axis.text = element_text(colour = "blue")) +
  theme(legend.justification = c(1, 0), 
        legend.position = c(1, 0),
        legend.box.margin=margin(c(50,50,50,50)))
#plot(TestROC)
```

# Slide 4: ROC Curves and AUC -- Train vs Test
```{r}
### ROC Curve train
plot(TrainROC)
### ROC Curve test
plot(TestROC)
```
  
```{r message=FALSE,warning=FALSE,include=FALSE}
### Area under the curve
AUC_results <- data.frame (
  TrainAUC = calc_auc(TrainROC),
  TestAUC =calc_auc(TestROC)
)

```
  
```{r}
### Area under the curve
AUC_results
```

# Slide 5: Logit Model Summary
## Both train and test set curves are above the diagonal chance-only line
## This means that determining whether wine is a "good value" is better than chance
## AUC values are high, above 80% or about 70% higher than chance
## AUC for train and test are nearly identical  (0.868 vs 0.867) hence model neither over- or underfit
## Points, Specific Province, Specific Variety, Specific Winery, followed by Specific Taster explain the model best whereas Title information not so much||||||| .r157
---
title: "LogitModel.RMD"
author: "Bart Ciastkowski"
date: "12/2/2019"
output:
  word_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
  
# The Code
### Adding libraries
```{r}
library("plyr")
library("conflicted")

library("tidyverse")
library("here")
library("ggthemes")
library("stringr")
library("stringi")
library("readxl")
library("ggExtra")
library("PerformanceAnalytics")
library("plotROC")
library('sentimentr')

```

### Resolving library conflicts
```{r}

conflict_prefer("mutate", "dplyr")
conflict_prefer("margin","ggplot2")
```
### Clear all objects including hidden objects.
```{r}
rm(list = ls(all.names = TRUE))
```
### Load Model and Train Data
```{r}
load(here::here("data","output","limited_factors","wine_train.RData"))
load(here::here("data","output","limited_factors","wine_test.RData"))
names(wine_train)
names(wine_test)
```
# Determining Good Value (i.e. well_priced)
### Need to consider diminishing returns, as the rate of marginal increase in points rating decreases with respect to the increase in the price of a bottle of wine, hence using log(price of wine):
##### Well_priced == whether we think wine is well priced
##### Well_priced will be determined according to a median price to points ratio computed as follows using observable from the training data set
##### median_price_to_points_ratio= Dataset ratio numerator  / Dataset ratio denominator
##### Dataset ratio numerator    = Median (points awarded)  - Min (points awarded)
##### Dataset ratio denominator  = Median ( log ( price of wine))  - Min (log (expected(min price of drinkable wine))
##### Where expected(min price of drinkable wine) = $2.5 (i.e. educated guess)
##### Where price of wine > expected (min price of drinkable wine)
### In Summary: the above formula takes into consideration diminishing returns i.e. marginal increase in points is accompanied by a higher and higher increase in price and assumes the lowest price for acceptable bottle of wine to be $2.5 (other numbers could be used in the future) 
```{r}
wine_train.pt.min <- min(wine_train$points) 
price.min <- 2.5

ratio.numerator <-  median(wine_train$points)-wine_train.pt.min
ratio.denominator <-  ifelse(wine_train$price<price.min, price.min + 0.1, wine_train$price)
ratio.denominator <- log(ratio.denominator)
ratio.denominator <-  median(ratio.denominator)
ratio.denominator <- ratio.denominator - log(price.min)
median_price_to_points_ratio <- ratio.numerator/ratio.denominator

.is_well_priced <- function(df){
  test.LHS.numerator <- df$points-wine_train.pt.min
  test.LHS.denominator <- ifelse(df$price<price.min, price.min + 0.1,df$price)
  test.LHS.denominator <- log(test.LHS.denominator)
  test.LHS.denominator <- test.LHS.denominator-log(price.min)
  test.LHS <- test.LHS.numerator/test.LHS.denominator
  test.RHS <- median_price_to_points_ratio
  well_priced <- factor(test.LHS > test.RHS, labels=c("No", "Yes"))
  return(well_priced)
}


# Compute well_priced for train ---- same formula as for median_price_to_points_ratio, except for an individual price point combination
wine_train_logit <- wine_train %>% 
  dplyr::mutate ( well_priced = .is_well_priced(.) )
```

## The following chart describes that relationship:
```{r}
ggplot(wine_train_logit , aes(x = price, y = points, color = well_priced)) +
  geom_jitter() +
  theme(legend.position = "top") + 
  labs(title="Price and Points Colored by Well Priced", 
       color = "Well Priced") 

head(wine_train)
```
### "Good Value" is about price, so we don't want it to be part of the model + removing points_category as using points
```{r}
wine_train_logit <- wine_train_logit %>% 
  dplyr::select(-price,-points.category)

# Compute well_priced for test dataset and remove price and points
wine_test_logit <- wine_test %>% 
  dplyr::mutate ( well_priced = .is_well_priced(.) )  %>% 
  select (-price,-points.category)

```

# Let's create the model ----
### Since well_priced is a binomial estimate of price, price is removed from the dataset (train and test) before model is created
### taster.twitter_handle and variety_and_color are also removed as they provide no added value (i.e. NA values)
### Otherwise same variables are used for the model so that it can be compared against other models
```{r}
logit_mod <- glm( well_priced ~ .,
                  data = wine_train_logit %>%   
                    select (
                      -taster.twitter_handle,
                      -variety_and_color, 
                    ),
                  family = binomial) #our varaible can be 0 or 1, a binomial
# summary of model ----
summary(logit_mod)
```

## Computing predictions
### Train test set
```{r}
preds.train<- data.frame (
  pred = predict(logit_mod, type="response"),
  wine_train_logit
  #actual = wine_train_logit$well_priced
) 
#head(preds.train)
```

### Test set
```{r}
preds.test<- data.frame (
  pred = predict(logit_mod, 
                 newdata=wine_test_logit, 
                 type="response"),
  wine_test_logit
  #actual = wine_test_logit$well_priced
) 
#head(preds.test)

```

## ROC Curve
```{r}
TrainDF <- data.frame(default = c(preds.train$well_priced),
                      scores = c(preds.train$pred),
                      models = c(rep("Train Data Set",length(preds.train$pred))))

#summary(TrainDF)

TestDF <- data.frame(default = c(preds.test$well_priced),
                     scores = c(preds.test$pred),
                     models = c(rep("Test Data Set",length(preds.test$pred))))

```
### ROC Curve train
```{r}
TrainROC <- ggplot(TrainDF, aes(m = scores, d = default, color = models)) + 
  geom_roc(show.legend = TRUE, labelsize = 3.5, cutoffs.at = c(.99,.9,.8,.7,.5,.3,.1,0))
TrainROC <- TrainROC + style_roc(theme = theme_grey) +
  theme(axis.text = element_text(colour = "blue")) +
  theme(legend.justification = c(1, 0), 
        legend.position = c(1, 0),
        legend.box.margin=margin(c(50,50,50,50)))
plot(TrainROC)
```
  
### ROC Curve test
```{r}
TestROC <- ggplot(TestDF, aes(m = scores, d = default, color = models)) + 
  geom_roc(show.legend = TRUE, labelsize = 3.5, cutoffs.at = c(.99,.9,.8,.7,.5,.3,.1,0))
TestROC <- TestROC + style_roc(theme = theme_grey) +
  theme(axis.text = element_text(colour = "blue")) +
  theme(legend.justification = c(1, 0), 
        legend.position = c(1, 0),
        legend.box.margin=margin(c(50,50,50,50)))
plot(TestROC)
```
  
### Area under the curve
```{r}
AUC_results <- data.frame (
  TrainAUC = calc_auc(TrainROC),
  TestAUC =calc_auc(TestROC)
)
AUC_results
```

# Logit Model Summary
## Both train and test set curves are above the diagonal chance-only line
## This means that determining whether wine is a "good value" is better than chance
## AUC values are high, above 80% or about 70% higher than chance
## AUC for train and test are nearly identical  (0.868 vs 0.867) hence model neither over- or underfit
## Points, Specific Province, Specific Variety, Specific Winery, followed by Specific Taster explain the model best whereas Title information not so much=======
---
title: "LogitModel.RMD"
author: "Bart Ciastkowski"
date: "12/2/2019"
output:
  word_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
  
# The Code
### Adding libraries
```{r}
library("plyr")
library("conflicted")
library("tidyverse")
library("here")
library("ggthemes")
library("stringr")
library("stringi")
library("readxl")
library("ggExtra")
library("PerformanceAnalytics")
library("plotROC")
library('sentimentr')
```

### Resolving library conflicts
```{r}
conflict_prefer("mutate", "dplyr")
conflict_prefer("margin","ggplot2")
```
### Clear all objects including hidden objects.
```{r}
rm(list = ls(all.names = TRUE))
```
### Load Model and Train Data
```{r}
load(here::here("data","output","limited_factors","wine_train.RData"))
load(here::here("data","output","limited_factors","wine_test.RData"))
names(wine_train)
names(wine_test)
```
# Determining Good Value (i.e. well_priced)
### Need to consider diminishing returns, as the rate of marginal increase in points rating decreases with respect to the increase in the price of a bottle of wine, hence using log(price of wine):
##### Well_priced == whether we think wine is well priced
##### Well_priced will be determined according to a median price to points ratio computed as follows using observable from the training data set
##### median_price_to_points_ratio= Dataset ratio numerator  / Dataset ratio denominator
##### Dataset ratio numerator    = Median (points awarded)  - Min (points awarded)
##### Dataset ratio denominator  = Median ( log ( price of wine))  - Min (log (expected(min price of drinkable wine))
##### Where expected(min price of drinkable wine) = $2.5 (i.e. educated guess)
##### Where price of wine > expected (min price of drinkable wine)
### In Summary: the above formula takes into consideration diminishing returns i.e. marginal increase in points is accompanied by a higher and higher increase in price and assumes the lowest price for acceptable bottle of wine to be $2.5 (other numbers could be used in the future) 
```{r}
wine_train.pt.min <- min(wine_train$points) 
price.min <- 2.5
ratio.numerator <-  median(wine_train$points)-wine_train.pt.min
ratio.denominator <-  ifelse(wine_train$price<price.min, price.min + 0.1, wine_train$price)
ratio.denominator <- log(ratio.denominator)
ratio.denominator <-  median(ratio.denominator)
ratio.denominator <- ratio.denominator - log(price.min)
median_price_to_points_ratio <- ratio.numerator/ratio.denominator
.is_well_priced <- function(df){
  test.LHS.numerator <- df$points-wine_train.pt.min
  test.LHS.denominator <- ifelse(df$price<price.min, price.min + 0.1,df$price)
  test.LHS.denominator <- log(test.LHS.denominator)
  test.LHS.denominator <- test.LHS.denominator-log(price.min)
  test.LHS <- test.LHS.numerator/test.LHS.denominator
  test.RHS <- median_price_to_points_ratio
  well_priced <- factor(test.LHS > test.RHS, labels=c("No", "Yes"))
  return(well_priced)
}
# Compute well_priced for train ---- same formula as for median_price_to_points_ratio, except for an individual price point combination
wine_train_logit <- wine_train %>% 
  dplyr::mutate ( well_priced = .is_well_priced(.) )
```

## The following chart describes that relationship:
```{r}
ggplot(wine_train_logit , aes(x = price, y = points, color = well_priced)) +
  geom_jitter() +
  theme(legend.position = "top") + 
  labs(title="Price and Points Colored by Well Priced", 
       color = "Well Priced") 
head(wine_train)
```
### "Good Value" is about price, so we don't want it to be part of the model + removing points_category as using points
```{r}
wine_train_logit <- wine_train_logit %>% 
  dplyr::select(-price,-points.category)
# Compute well_priced for test dataset and remove price and points
wine_test_logit <- wine_test %>% 
  dplyr::mutate ( well_priced = .is_well_priced(.) )  %>% 
  select (-price,-points.category)
```

# Let's create the model ----
### Since well_priced is a binomial estimate of price, price is removed from the dataset (train and test) before model is created
### taster.twitter_handle and variety_and_color are also removed as they provide no added value (i.e. NA values)
### Otherwise same variables are used for the model so that it can be compared against other models
```{r}
logit_mod <- glm( well_priced ~ .,
                  data = wine_train_logit %>%   
                    select (
                      -taster.twitter_handle,
                      -variety_and_color, 
                    ),
                  family = binomial) #our varaible can be 0 or 1, a binomial
# summary of model ----
summary(logit_mod)
```

## Computing predictions
### Train test set
```{r}
preds.train<- data.frame (
  pred = predict(logit_mod, type="response"),
  wine_train_logit
  #actual = wine_train_logit$well_priced
) 
#head(preds.train)
```

### Test set
```{r}
preds.test<- data.frame (
  pred = predict(logit_mod, 
                 newdata=wine_test_logit, 
                 type="response"),
  wine_test_logit
  #actual = wine_test_logit$well_priced
) 
#head(preds.test)
```

## ROC Curve
```{r}
TrainDF <- data.frame(default = c(preds.train$well_priced),
                      scores = c(preds.train$pred),
                      models = c(rep("Train Data Set",length(preds.train$pred))))
#summary(TrainDF)
TestDF <- data.frame(default = c(preds.test$well_priced),
                     scores = c(preds.test$pred),
                     models = c(rep("Test Data Set",length(preds.test$pred))))
```
### ROC Curve train
```{r}
TrainROC <- ggplot(TrainDF, aes(m = scores, d = default, color = models)) + 
  geom_roc(show.legend = TRUE, labelsize = 3.5, cutoffs.at = c(.99,.9,.8,.7,.5,.3,.1,0))
TrainROC <- TrainROC + style_roc(theme = theme_grey) +
  theme(axis.text = element_text(colour = "blue")) +
  theme(legend.justification = c(1, 0), 
        legend.position = c(1, 0),
        legend.box.margin=margin(c(50,50,50,50)))
plot(TrainROC)
```
  
### ROC Curve test
```{r}
TestROC <- ggplot(TestDF, aes(m = scores, d = default, color = models)) + 
  geom_roc(show.legend = TRUE, labelsize = 3.5, cutoffs.at = c(.99,.9,.8,.7,.5,.3,.1,0))
TestROC <- TestROC + style_roc(theme = theme_grey) +
  theme(axis.text = element_text(colour = "blue")) +
  theme(legend.justification = c(1, 0), 
        legend.position = c(1, 0),
        legend.box.margin=margin(c(50,50,50,50)))
plot(TestROC)
```
  
### Area under the curve
```{r}
AUC_results <- data.frame (
  TrainAUC = calc_auc(TrainROC),
  TestAUC =calc_auc(TestROC)
)
AUC_results
```

# Logit Model Summary
## Both train and test set curves are above the diagonal chance-only line
## This means that determining whether wine is a "good value" is better than chance
## AUC values are high, above 80% or about 70% higher than chance
## AUC for train and test are nearly identical  (0.868 vs 0.867) hence model neither over- or underfit
## Points, Specific Province, Specific Variety, Specific Winery, followed by Specific Taster explain the model best whereas Title informat>>>>>>> .r166
