---
title: "test"
author: "Bart Ciastkowski"
date: "12/3/2019"
output: html_document
---

# Bootstrap Model

# Slide 1 Modelling log(price) 

* Method: using decision-tree via Bootstrap technique
* set number of Bootstrap samples to 100 
* set size of each bootstrap to 500 to ensure adequate data

# Slide 2: Example 5 decision tree

```{r}
  plot(boot_mods[[5]])
```

# Slide 3: Actual Price vs  Predicted Price

```{r}
  ggplot(wine_train, aes(x = log(price))) + xlim(0, 10) + geom_histogram(binwidth=0.1)

  ggplot(wine_train_preds, aes(x = preds_bag)) + xlim(0, 10) + geom_histogram(binwidth=0.1)
```

# Slide 4: Summary of Boot strap
* variations of bootstrap predictions appear to match training dataset
* bootstrap predictions have a bias, a tendency to undervalue wine
* most root and decision nodes use points for decision to split, then taster.avg_points and variety are used


# Random Forest

# Slide 1: Modelling log(price) using R Forest

* using reduced data size -- lack of computational horsepower
* purpose: modelling log(price) that generates a set of bootstrapp trees (i.e. forests) and combine results via average regresion 


# Slide 2: Summary of Results

* Number of trees: 500
* Mean of squared residuals: 0.1756748
* Var explained: 61.2


# Slide 3: Gini Coefficient

* %IncMSE - higher means more important 
* IncNodePurity - Total decrease in node impurities from splitting on the variable, averaged over all trees. 

```{r}
  varImpPlot(rf_fit,type=1)

  varImpPlot(rf_fit,type=2)
```


# Slide 4: R Forest Explainer

* Title n chars  vs  points
plot_predict_interaction(rf_fit, wine_train, "title.n_chars", "points")
*  Title n words  vs  points
plot_predict_interaction(rf_fit, wine_train, "title.n_words", "points")
* taster avg_points  vs  points
plot_predict_interaction(rf_fit, wine_train, "taster.avg_points", "points")


# Logit Model

# Slide 1: Determining whether wine good value or not
* Logit Model
** logit_mod <- glm( well_priced ~ …… )
* How is Well_Priced determined?
** Well_priced == whether we think wine is well priced
** Well_priced == f(median log(price) to points ratio)
* In Summary: Well_Priced takes into consideration diminishing returns i.e. marginal increase in points is accompanied by a higher and higher increase in price

# Slide 2: Points vs Price by Well Priced

* Purpose: showing the effects of dimishing returns
```{r}
PointsVsPricePlot <-ggplot(wine_train_logit , aes(x = price, y = points, color = well_priced)) +
  geom_jitter() +
  theme(legend.position = "top") + 
  labs(title="Price and Points Colored by Well Priced", 
       color = "Well Priced")

PointsVsPricePlot
```

# Slide 3: Let’s create the model
* Notes: 
** Price removed from the dataset 
** variety_and_color removed as it provided no added value (i.e. NA values)
* logit_mod <- glm( well_priced ~ ., 

# Slide 4: ROC Curves and AUC – Train vs Test
```{r}
plot(TrainROC)
plot(TestROC)
AUC_results
```

# Slide 5: Logit Model Summary
* train and test above diagonal chance-only line
* determining whether wine is a “good value” better than chance
* AUC values are high, above 80% or about 70% higher than chance
* AUC for train and test  nearly identical hence model neither over- or underfit
* Points, Specific Province, Specific Variety, Specific Winery, followed by Specific Taster explain the model best whereas Title information not so much


